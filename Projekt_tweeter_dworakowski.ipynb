{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-k_9KrX6XEu",
        "outputId": "8cbec7bb-1f04-4230-efeb-8762b55182b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stop-words in /usr/local/lib/python3.7/dist-packages (2018.7.23)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "\n",
            "\n",
            "Czy włączyć szybkie uruchomienie? Jeśli tak tweety zostaną wczytane z pliku, a nie pobrane z API. Odpowiedź tak / nie: nie\n",
            "Przy pobieraniu z API w lokalizacji notatnika powinien znaleźc się plik token.txt z bearer tokenem dostępowym do API\n",
            "{'data': {'id': '1221826367928655876', 'text': 'MSZ przygotowuje ewakuację Polaków z chińskiego Wuhan https://t.co/Mk9D4bS8Ob #wieszwiecej'}}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "!pip install stop-words\n",
        "from stop_words import get_stop_words\n",
        "import io\n",
        "import os\n",
        "!pip install wget\n",
        "import wget\n",
        "import shutil\n",
        "from tensorflow import keras\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, RocCurveDisplay, plot_roc_curve, f1_score\n",
        "import re\n",
        "import spacy\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import tensorflow as tf\n",
        "\n",
        "fast_run = input('\\n\\nCzy włączyć szybkie uruchomienie? Jeśli tak tweety zostaną wczytane z pliku, a nie pobrane z API. Odpowiedź tak / nie: ').lower()\n",
        "\n",
        "if not (fast_run == 'tak' or fast_run == 'nie'):\n",
        "    raise TypeError('odpowiedź powinna brzmieć tak / nie')\n",
        "\n",
        "fast_run = True if fast_run == 'tak' else False\n",
        "\n",
        "if not fast_run: # test\n",
        "    print('Przy pobieraniu z API w lokalizacji notatnika powinien znaleźc się plik token.txt z bearer tokenem dostępowym do API')\n",
        "    with open('token.txt', 'r') as file:\n",
        "        token = file.read()\n",
        "\n",
        "    headers = {'Authorization': f'Bearer {token}'}\n",
        "    response = requests.get('https://api.twitter.com/2/tweets/1221826367928655876', headers=headers)\n",
        "    print(response.json()) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API blokuje mnie za każdym razem po pobraniu 25 - 30 tysięcy tweetów z informacją że limit został przekroczony. Dlatego pobieram tyle, na ile limit pozwala. Nie ma to jednak wpływu na jakośc zadania - wyniki i tak są dobre"
      ],
      "metadata": {
        "id": "0NYYmY1ge6lp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w1yDAwwqBapa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1737aa-7cda-4b6b-dbd3-11d42bfb47ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba przygotowanych id tweetow: 50000\n",
            "5000 tweets ids processed\n",
            "10000 tweets ids processed\n",
            "15000 tweets ids processed\n",
            "20000 tweets ids processed\n",
            "25000 tweets ids processed\n",
            "30000 tweets ids processed\n",
            "Limit reached. Quiting loop\n",
            "\n",
            "Liczba pobranych tweetów: 22135\n"
          ]
        }
      ],
      "source": [
        "def request_tweets_from_api():\n",
        "    url = 'https://raw.githubusercontent.com/PatrycyD/ZUM_NLP/master/pl_covid_tweets_clean.txt'\n",
        "    wget.download(url) if not os.path.isfile('pl_covid_tweets_clean.txt') else 0\n",
        "\n",
        "    i = 0\n",
        "    with open('pl_covid_tweets_clean.txt', 'r') as file:\n",
        "        tweets_ids = [line.split('\\t')[0] for line in file]\n",
        "\n",
        "    tweets_ids = tweets_ids[1:] #pominięcie nagłówka\n",
        "    tweets_ids = tweets_ids[:50000]\n",
        "\n",
        "    print(f'Liczba przygotowanych id tweetow: {len(tweets_ids)}')\n",
        "\n",
        "    tweets = {}\n",
        "    i = 0\n",
        "    for tweet_ids_list in range(0, len(tweets_ids), 100):\n",
        "        i = i + 100\n",
        "        group_ids = ','.join(tweets_ids[tweet_ids_list:tweet_ids_list+100])\n",
        "        r = requests.get(f'https://api.twitter.com/2/tweets?ids={group_ids}', headers=headers)\n",
        "        if r.status_code == 429:\n",
        "          print('Limit reached. Quiting loop')\n",
        "          break\n",
        "          # raise OverflowError('API requests limit reached. Pleas try again in a few minutes')\n",
        "        try:\n",
        "            for tweet in dict(r.json())['data']:\n",
        "                tweets[tweet['id']] = tweet['text']\n",
        "        except KeyError: #gdy wszystkie pobrany teety nie istnieją, nie ma klucza 'data'\n",
        "            continue\n",
        "\n",
        "        \n",
        "        if i % 5000 == 0:\n",
        "            print(f'{i} tweets ids processed')\n",
        "\n",
        "    print(f'\\nLiczba pobranych tweetów: {len(tweets)}')\n",
        "    \n",
        "    with open('/content/drive/MyDrive/tweets.json', 'w', encoding='utf-8') as file:\n",
        "        file.write(json.dumps(tweets))\n",
        "    \n",
        "    return tweets\n",
        "\n",
        "def load_tweets_from_repo():\n",
        "    url = 'https://raw.githubusercontent.com/PatrycyD/ZUM_NLP/master/tweets.json'\n",
        "    wget.download(url) if not os.path.isfile('tweets.json') else 0\n",
        "    file = open('tweets.json', 'r', encoding=\"utf-8\")\n",
        "    tweets = file.read()\n",
        "    tweets = tweets.replace(\"'\", '\"')\n",
        "    tweets = json.loads(tweets)\n",
        "    \n",
        "    return tweets\n",
        "    \n",
        "if fast_run:\n",
        "    tweets = load_tweets_from_repo()\n",
        "else:\n",
        "    tweets = request_tweets_from_api()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2Ghe9oxGCrs",
        "outputId": "c8aec123-16e7-4f14-c59d-200cbd9f4da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Collecting pl-core-news-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_sm-3.2.0/pl_core_news_sm-3.2.0-py3-none-any.whl (58.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 58.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pl-core-news-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->pl-core-news-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pl_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "def cleaning_URLs(data):\n",
        "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
        "\n",
        "punctuations_list = string.punctuation\n",
        "def cleaning_punctuations(text):\n",
        "    translator = str.maketrans('', '', punctuations_list) # jest to mapowanie i zamiana, dwa pierwsze argumenty to dwa stringi, gdzie znaki w pierwszym są zamieniane na znaki w drugim stringu, zgodnie z indeksem, Trzeci argument to znaki, które są mapowane do None => zostaną po prostu usunięte\n",
        "    return text.translate(translator)\n",
        "\n",
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r'(.)1+', r'1', text)\n",
        "\n",
        "def clean_numbers(data):\n",
        "    return re.sub('[0-9]+', '',\n",
        "                  data)\n",
        "\n",
        "!pip install --upgrade spacy\n",
        "!python -m spacy download pl_core_news_sm\n",
        "nlp = spacy.load('pl_core_news_sm')\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    lemmatized_sentence = ''\n",
        "    for token in doc:\n",
        "        lemmatized_sentence = f'{lemmatized_sentence} {token.lemma_}'\n",
        "        \n",
        "    return lemmatized_sentence.strip()\n",
        "\n",
        "stop_words = get_stop_words('polish')\n",
        "no_stop_words_tweets = {key: ' '.join([word for word in value.split() if word \n",
        "                                       not in stop_words]) for key, value in tweets.items()}\n",
        "\n",
        "tweets_no_urls = {k: cleaning_URLs(v) for k, v in no_stop_words_tweets.items()}\n",
        "tweets_no_punct = {k: cleaning_punctuations(v) for k, v in no_stop_words_tweets.items()}\n",
        "tweets_no_repeating_chars = {k: cleaning_repeating_char(v) for k, v in tweets_no_punct.items()}\n",
        "tweets_no_numbers = {k: clean_numbers(v) for k, v in tweets_no_urls.items()}\n",
        "tweets_lemmatized = {k: lemmatize(v) for k, v in tweets_no_numbers.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euSY6NHFGCrt",
        "outputId": "58ccdcf7-b570-485c-b47b-fa12e2550ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 933198 word vectors.\n"
          ]
        }
      ],
      "source": [
        "embedding_file = 'pl-embeddings-cbow.txt'\n",
        "\n",
        "if not os.path.isfile(embedding_file):\n",
        "    url = 'http://publications.ics.p.lodz.pl/2016/word_embeddings/pl-embeddings-cbow.txt'\n",
        "    wget.download(url)\n",
        "    \n",
        "embeddings_index = {}\n",
        "with open(embedding_file, encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "        embeddings_index[word] = coefs\n",
        "        \n",
        "embeddings_index.pop('933198') #na początku pliku jest notatka o liczbie wektorów i liczbie punktów w wektorach\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAosf1X1GCru",
        "outputId": "8b8b80df-ff65-463e-a4a2-ac74ccb92af7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2599,   92,   11,   62,  531,    8])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = [v for k, v in tweets_lemmatized.items()]\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "output = vectorizer([['szczepienie choroba wirus chory zdrowy koronawirus']])\n",
        "output.numpy()[0, :6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77sLzlO7GCrv",
        "outputId": "3737b027-b600-4296-8b2a-911bb675b958"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2599, 92, 11, 62, 531, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n",
        "\n",
        "test = ['szczepienie', 'choroba', 'wirus', 'chory', 'zdrowy', 'koronawirus']\n",
        "[word_index[w] for w in test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAzc5qSoGCrv",
        "outputId": "69406ae7-a174-4870-d239-a3a7e9421aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 14876 words (5124 misses)\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "74hm4wCGGCrx"
      },
      "outputs": [],
      "source": [
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(embedding_matrix)\n",
        "classes = kmeans.predict(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZEZuBnpEGCrz"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(embedding_matrix, classes, test_size=0.1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=15, max_depth=5)\n",
        "knn = KNeighborsClassifier(n_neighbors=20)\n",
        "svc = SVC()\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "knn.fit(X_train, y_train)\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "rf_preds = rf.predict(X_test)\n",
        "knn_preds = knn.predict(X_test)\n",
        "svc_preds = svc.predict(X_test)\n",
        "\n",
        "rf_train_preds = rf.predict(X_train)\n",
        "knn_train_preds = knn.predict(X_train)\n",
        "svc_train_preds = svc.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpQqlRW_GCr0",
        "outputId": "94d1567e-a669-44f4-e13d-df5c5d75b208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest test predictions\n",
            "[[1237    0   32]\n",
            " [  24  160   13]\n",
            " [ 166    3  366]]\n",
            "\n",
            "\n",
            "K Nearest Neighbors test predictions\n",
            "[[1222    0   47]\n",
            " [  10  186    1]\n",
            " [ 163    1  371]]\n",
            "\n",
            "\n",
            "Support Vector Machines test predictions\n",
            "[[1254    3   12]\n",
            " [   1  196    0]\n",
            " [  12    1  522]]\n"
          ]
        }
      ],
      "source": [
        "print('Random Forest test predictions')\n",
        "print(confusion_matrix(y_test, rf_preds))\n",
        "print('\\n\\nK Nearest Neighbors test predictions')\n",
        "print(confusion_matrix(y_test, knn_preds))\n",
        "print('\\n\\nSupport Vector Machines test predictions')\n",
        "print(confusion_matrix(y_test, svc_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1LAK_p3GCr0",
        "outputId": "e4273ed0-7230-4a0c-fa6b-105e7d1ac13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest train predictions\n",
            "[[11397     3   256]\n",
            " [  162  1440    76]\n",
            " [ 1460     7  3200]]\n",
            "\n",
            "\n",
            "K Nearest Neighbors train predictions\n",
            "[[11281     4   371]\n",
            " [   51  1622     5]\n",
            " [ 1312     3  3352]]\n",
            "\n",
            "\n",
            "Support Vector Machines train predictions\n",
            "[[11650     1     5]\n",
            " [    0  1678     0]\n",
            " [    3     1  4663]]\n"
          ]
        }
      ],
      "source": [
        "print('Random Forest train predictions')\n",
        "print(confusion_matrix(y_train, rf_train_preds))\n",
        "print('\\n\\nK Nearest Neighbors train predictions')\n",
        "print(confusion_matrix(y_train, knn_train_preds))\n",
        "print('\\n\\nSupport Vector Machines train predictions')\n",
        "print(confusion_matrix(y_train, svc_train_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbwcGzV0GCr1"
      },
      "source": [
        "### Nie mogę użyć ROC do ewaluacji modelu, ponieważ klasyfikacja jest niebinarna, użyję F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzMqZX3YGCr3",
        "outputId": "8ca8bb5f-0422-4fb9-8bef-8d6c0d77bfab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranom Forest test f1: 0.86\n",
            "Ranom Forest train f1: 0.88\n",
            "\n",
            "K Nearest Neighbors test f1: 0.89\n",
            "K Nearest Neighbors train f1: 0.9\n",
            "\n",
            "Support Vector Machines test f1: 0.98\n",
            "Support Vector Machines train f1: 1.0\n"
          ]
        }
      ],
      "source": [
        "print(f'Ranom Forest test f1: {round(f1_score(y_test, rf_preds, average=\"macro\"), 2)}')\n",
        "print(f'Ranom Forest train f1: {round(f1_score(y_train, rf_train_preds, average=\"macro\"), 2)}')\n",
        "\n",
        "print(f'\\nK Nearest Neighbors test f1: {round(f1_score(y_test, knn_preds, average=\"macro\"), 2)}')\n",
        "print(f'K Nearest Neighbors train f1: {round(f1_score(y_train, knn_train_preds, average=\"macro\"), 2)}')\n",
        "\n",
        "print(f'\\nSupport Vector Machines test f1: {round(f1_score(y_test, svc_preds, average=\"macro\"), 2)}')\n",
        "print(f'Support Vector Machines train f1: {round(f1_score(y_train, svc_train_preds, average=\"macro\"), 2)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce8qI727GCr3"
      },
      "source": [
        "### Wyniki są dobre"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sprzątanie:\n",
        "!rm pl-embeddings-cbow.txt\n",
        "!rm pl_covid_tweets_clean.txt\n",
        "![[ -f tweets.json ]] && rm tweets.json"
      ],
      "metadata": {
        "id": "K_wOH2MGfii_"
      },
      "execution_count": 28,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Projekt_tweeter_dworakowski.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}